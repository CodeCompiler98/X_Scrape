{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install googletrans==4.0.0-rc1 --quiet\n",
    "%pip install pandas --quiet\n",
    "%pip install nltk --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from googletrans import Translator\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Post ID                                       URL to tweet  \\\n",
      "0  1828837775791341997  https://x.com/JimmyMatthewsJr/status/182883777...   \n",
      "1  1828607916892422511  https://x.com/JimmyMatthewsJr/status/182860791...   \n",
      "2  1826979750856049026  https://x.com/TheVladsven/status/1826979750856...   \n",
      "3  1822071851960692972  https://x.com/TheVladsven/status/1822071851960...   \n",
      "4  1822069995565883579  https://x.com/TheVladsven/status/1822069995565...   \n",
      "5  1828212669670629624  https://x.com/Bloomer99999/status/182821266967...   \n",
      "6  1828060378711945505  https://x.com/EmilyMach5/status/18280603787119...   \n",
      "7  1813345368484073626  https://x.com/ScottWIsAnIdiot/status/181334536...   \n",
      "8  1829862469894058286  https://x.com/EmilyMach5/status/18298624698940...   \n",
      "9  1823835478480605640  https://x.com/acnewsitics/status/1823835478480...   \n",
      "\n",
      "                    creation_date  likes  \\\n",
      "0  Wed Aug 28 16:51:18 +0000 2024     29   \n",
      "1  Wed Aug 28 01:37:55 +0000 2024     21   \n",
      "2  Fri Aug 23 13:48:10 +0000 2024      1   \n",
      "3  Sat Aug 10 00:45:56 +0000 2024     19   \n",
      "4  Sat Aug 10 00:38:34 +0000 2024      8   \n",
      "5  Mon Aug 26 23:27:21 +0000 2024      2   \n",
      "6  Mon Aug 26 13:22:12 +0000 2024     17   \n",
      "7  Tue Jul 16 22:50:00 +0000 2024     10   \n",
      "8  Sat Aug 31 12:43:04 +0000 2024     54   \n",
      "9  Wed Aug 14 21:33:57 +0000 2024  56668   \n",
      "\n",
      "                                             caption  retweet_count  \\\n",
      "0  #TrumpIsUnfitForOffice #ArlingtonNationalCemet...             18   \n",
      "1  @thedesertginger I would not be surprised if h...              5   \n",
      "2  Respect marriage.  Donald Trump and Where's Me...              2   \n",
      "3  Pray for Trump.  He longs for North Korea and ...              3   \n",
      "4  Pray for Trump. He joined Hamas.   ðŸ‡ºðŸ‡¸ #trumpme...              2   \n",
      "5         #KamalaWinswithJoy https://t.co/k5dnunB1zQ              0   \n",
      "6  @VetsForRL Trump has used everyone around him ...             11   \n",
      "7  @ZionDarkwood @SteveLovesAmmo https://t.co/rJu...              5   \n",
      "8  @SteveSchmidtSES #TrumpDishonorsUS #TrumpIsUnf...             10   \n",
      "9        Cop vs the felon. ðŸ˜‚ https://t.co/LfXb1J7bBA           7448   \n",
      "\n",
      "   comment_count  view_count    poster_handle        poster_display_name  \\\n",
      "0              5        1793  JimmyMatthewsJr             Jimmy Matthews   \n",
      "1              1         863  JimmyMatthewsJr             Jimmy Matthews   \n",
      "2              2        1108      TheVladsven  Darth VladSven the Cat VI   \n",
      "3              4        3808      TheVladsven  Darth VladSven the Cat VI   \n",
      "4              2        2869      TheVladsven  Darth VladSven the Cat VI   \n",
      "5              0          58     Bloomer99999            Nabster ðŸ’™ðŸ‡ºðŸ‡¸ðŸ¦…ðŸ‡ºðŸ‡¸ðŸ’™   \n",
      "6              3         898       EmilyMach5                 Emily Mach   \n",
      "7              6        9438  ScottWIsAnIdiot   Scott Walker is An Idiot   \n",
      "8              1        1216       EmilyMach5                 Emily Mach   \n",
      "9           1546     1981822      acnewsitics                  Alex Cole   \n",
      "\n",
      "   poster_follower_count  poster_frequency  total_posts  \\\n",
      "0                   2613          5.376404        30624   \n",
      "1                   2613          5.376404        30624   \n",
      "2                    955         26.451581        26769   \n",
      "3                    955         26.451581        26769   \n",
      "4                    955         26.451581        26769   \n",
      "5                   5168         63.658373        66523   \n",
      "6                    397          1.938092         3068   \n",
      "7                   2476         14.871070        37371   \n",
      "8                    397          1.938092         3068   \n",
      "9                 283007         18.049858        38013   \n",
      "\n",
      "                                            hashtags  \\\n",
      "0  #TrumpIsUnfitForOffice, #ArlingtonNationalCeme...   \n",
      "1                             #TrumpIsUnfitForOffice   \n",
      "2  #TrumpIsDone, #KamalaHarris, #Kamala4President...   \n",
      "3                           #trumprally, #Kamala2024   \n",
      "4                           #trumprally, #Kamala2024   \n",
      "5                                 #KamalaWinswithJoy   \n",
      "6  #TrumpIsUnfitForOffice, #VeteransForHarrisWalz...   \n",
      "7                                                NaN   \n",
      "8          #TrumpDishonorsUS, #TrumpIsUnfitForOffice   \n",
      "9                                                NaN   \n",
      "\n",
      "                                      image_filename  \\\n",
      "0  Twitter_Scrape\\Target_Trump\\Anti\\Images\\eat_on...   \n",
      "1  Twitter_Scrape\\Target_Trump\\Anti\\Images\\vetera...   \n",
      "2  Twitter_Scrape\\Target_Trump\\Anti\\Images\\trump_...   \n",
      "3  Twitter_Scrape\\Target_Trump\\Anti\\Images\\trump_...   \n",
      "4  Twitter_Scrape\\Target_Trump\\Anti\\Images\\foreig...   \n",
      "5  Twitter_Scrape\\Target_Trump\\Anti\\Images\\trump_...   \n",
      "6  Twitter_Scrape\\Target_Trump\\Anti\\Images\\trump_...   \n",
      "7  Twitter_Scrape\\Target_Harris\\Anti\\Images\\setup...   \n",
      "8  Twitter_Scrape\\Target_Trump\\Anti\\Images\\trump_...   \n",
      "9  Twitter_Scrape\\Target_Trump\\Anti\\Images\\trump_...   \n",
      "\n",
      "                                        comment_file image+caption_target  \\\n",
      "0  Twitter_Scrape\\Target_Trump\\Anti\\Comments\\eat_...           anti-trump   \n",
      "1  Twitter_Scrape\\Target_Trump\\Anti\\Comments\\vete...           anti-trump   \n",
      "2                                                NaN           anti-trump   \n",
      "3  Twitter_Scrape\\Target_Trump\\Anti\\Comments\\trum...           anti-trump   \n",
      "4  Twitter_Scrape\\Target_Trump\\Anti\\Comments\\fore...           anti-trump   \n",
      "5                                                NaN           anti-trump   \n",
      "6  Twitter_Scrape\\Target_Trump\\Anti\\Comments\\trum...           anti-trump   \n",
      "7  Twitter_Scrape\\Target_Harris\\Anti\\Comments\\set...          anti-harris   \n",
      "8  Twitter_Scrape\\Target_Trump\\Anti\\Comments\\trum...           anti-trump   \n",
      "9  Twitter_Scrape\\Target_Trump\\Anti\\Comments\\trum...           anti-trump   \n",
      "\n",
      "  political_leaning  \n",
      "0                 D  \n",
      "1                 D  \n",
      "2                 D  \n",
      "3                 D  \n",
      "4                 D  \n",
      "5                 D  \n",
      "6                 D  \n",
      "7                 R  \n",
      "8                 D  \n",
      "9                 D  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the absolute path to the CSV file\n",
    "csv_file_path = r'C:\\Users\\Jonah Dalton\\Data_Pipeline\\Twitter_Scrape\\tweets.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "try:\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    # Display the DataFrame\n",
    "    print(df.head(10))\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {csv_file_path}\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"The file is empty.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Post ID                                       URL to tweet  \\\n",
      "0  1828837775791341997  https://x.com/JimmyMatthewsJr/status/182883777...   \n",
      "1  1828607916892422511  https://x.com/JimmyMatthewsJr/status/182860791...   \n",
      "2  1826979750856049026  https://x.com/TheVladsven/status/1826979750856...   \n",
      "3  1822071851960692972  https://x.com/TheVladsven/status/1822071851960...   \n",
      "4  1822069995565883579  https://x.com/TheVladsven/status/1822069995565...   \n",
      "5  1828212669670629624  https://x.com/Bloomer99999/status/182821266967...   \n",
      "6  1828060378711945505  https://x.com/EmilyMach5/status/18280603787119...   \n",
      "7  1813345368484073626  https://x.com/ScottWIsAnIdiot/status/181334536...   \n",
      "8  1829862469894058286  https://x.com/EmilyMach5/status/18298624698940...   \n",
      "9  1823835478480605640  https://x.com/acnewsitics/status/1823835478480...   \n",
      "\n",
      "                    creation_date  likes  \\\n",
      "0  Wed Aug 28 16:51:18 +0000 2024     29   \n",
      "1  Wed Aug 28 01:37:55 +0000 2024     21   \n",
      "2  Fri Aug 23 13:48:10 +0000 2024      1   \n",
      "3  Sat Aug 10 00:45:56 +0000 2024     19   \n",
      "4  Sat Aug 10 00:38:34 +0000 2024      8   \n",
      "5  Mon Aug 26 23:27:21 +0000 2024      2   \n",
      "6  Mon Aug 26 13:22:12 +0000 2024     17   \n",
      "7  Tue Jul 16 22:50:00 +0000 2024     10   \n",
      "8  Sat Aug 31 12:43:04 +0000 2024     54   \n",
      "9  Wed Aug 14 21:33:57 +0000 2024  56668   \n",
      "\n",
      "                                    original_caption  retweet_count  \\\n",
      "0  #TrumpIsUnfitForOffice #ArlingtonNationalCemet...             18   \n",
      "1  @thedesertginger I would not be surprised if h...              5   \n",
      "2  Respect marriage.  Donald Trump and Where's Me...              2   \n",
      "3  Pray for Trump.  He longs for North Korea and ...              3   \n",
      "4  Pray for Trump. He joined Hamas.   ðŸ‡ºðŸ‡¸ #trumpme...              2   \n",
      "5         #KamalaWinswithJoy https://t.co/k5dnunB1zQ              0   \n",
      "6  @VetsForRL Trump has used everyone around him ...             11   \n",
      "7  @ZionDarkwood @SteveLovesAmmo https://t.co/rJu...              5   \n",
      "8  @SteveSchmidtSES #TrumpDishonorsUS #TrumpIsUnf...             10   \n",
      "9        Cop vs the felon. ðŸ˜‚ https://t.co/LfXb1J7bBA           7448   \n",
      "\n",
      "   comment_count  view_count    poster_handle        poster_display_name  ...  \\\n",
      "0              5        1793  JimmyMatthewsJr             Jimmy Matthews  ...   \n",
      "1              1         863  JimmyMatthewsJr             Jimmy Matthews  ...   \n",
      "2              2        1108      TheVladsven  Darth VladSven the Cat VI  ...   \n",
      "3              4        3808      TheVladsven  Darth VladSven the Cat VI  ...   \n",
      "4              2        2869      TheVladsven  Darth VladSven the Cat VI  ...   \n",
      "5              0          58     Bloomer99999            Nabster ðŸ’™ðŸ‡ºðŸ‡¸ðŸ¦…ðŸ‡ºðŸ‡¸ðŸ’™  ...   \n",
      "6              3         898       EmilyMach5                 Emily Mach  ...   \n",
      "7              6        9438  ScottWIsAnIdiot   Scott Walker is An Idiot  ...   \n",
      "8              1        1216       EmilyMach5                 Emily Mach  ...   \n",
      "9           1546     1981822      acnewsitics                  Alex Cole  ...   \n",
      "\n",
      "   poster_frequency  total_posts  \\\n",
      "0          5.376404        30624   \n",
      "1          5.376404        30624   \n",
      "2         26.451581        26769   \n",
      "3         26.451581        26769   \n",
      "4         26.451581        26769   \n",
      "5         63.658373        66523   \n",
      "6          1.938092         3068   \n",
      "7         14.871070        37371   \n",
      "8          1.938092         3068   \n",
      "9         18.049858        38013   \n",
      "\n",
      "                                            hashtags  \\\n",
      "0  #TrumpIsUnfitForOffice, #ArlingtonNationalCeme...   \n",
      "1                             #TrumpIsUnfitForOffice   \n",
      "2  #TrumpIsDone, #KamalaHarris, #Kamala4President...   \n",
      "3                           #trumprally, #Kamala2024   \n",
      "4                           #trumprally, #Kamala2024   \n",
      "5                                 #KamalaWinswithJoy   \n",
      "6  #TrumpIsUnfitForOffice, #VeteransForHarrisWalz...   \n",
      "7                                                      \n",
      "8          #TrumpDishonorsUS, #TrumpIsUnfitForOffice   \n",
      "9                                                      \n",
      "\n",
      "                                      image_filename  \\\n",
      "0  Twitter_Scrape\\Target_Trump\\Anti\\Images\\eat_on...   \n",
      "1  Twitter_Scrape\\Target_Trump\\Anti\\Images\\vetera...   \n",
      "2  Twitter_Scrape\\Target_Trump\\Anti\\Images\\trump_...   \n",
      "3  Twitter_Scrape\\Target_Trump\\Anti\\Images\\trump_...   \n",
      "4  Twitter_Scrape\\Target_Trump\\Anti\\Images\\foreig...   \n",
      "5  Twitter_Scrape\\Target_Trump\\Anti\\Images\\trump_...   \n",
      "6  Twitter_Scrape\\Target_Trump\\Anti\\Images\\trump_...   \n",
      "7  Twitter_Scrape\\Target_Harris\\Anti\\Images\\setup...   \n",
      "8  Twitter_Scrape\\Target_Trump\\Anti\\Images\\trump_...   \n",
      "9  Twitter_Scrape\\Target_Trump\\Anti\\Images\\trump_...   \n",
      "\n",
      "                                        comment_file image+caption_target  \\\n",
      "0  Twitter_Scrape\\Target_Trump\\Anti\\Comments\\eat_...           anti-trump   \n",
      "1  Twitter_Scrape\\Target_Trump\\Anti\\Comments\\vete...           anti-trump   \n",
      "2                                                NaN           anti-trump   \n",
      "3  Twitter_Scrape\\Target_Trump\\Anti\\Comments\\trum...           anti-trump   \n",
      "4  Twitter_Scrape\\Target_Trump\\Anti\\Comments\\fore...           anti-trump   \n",
      "5                                                NaN           anti-trump   \n",
      "6  Twitter_Scrape\\Target_Trump\\Anti\\Comments\\trum...           anti-trump   \n",
      "7  Twitter_Scrape\\Target_Harris\\Anti\\Comments\\set...          anti-harris   \n",
      "8  Twitter_Scrape\\Target_Trump\\Anti\\Comments\\trum...           anti-trump   \n",
      "9  Twitter_Scrape\\Target_Trump\\Anti\\Comments\\trum...           anti-trump   \n",
      "\n",
      "  political_leaning                       mentions urls  \\\n",
      "0                 D                                       \n",
      "1                 D               @thedesertginger        \n",
      "2                 D                                       \n",
      "3                 D                                       \n",
      "4                 D                                       \n",
      "5                 D                                       \n",
      "6                 D                     @VetsForRL        \n",
      "7                 R  @ZionDarkwood @SteveLovesAmmo        \n",
      "8                 D               @SteveSchmidtSES        \n",
      "9                 D                                       \n",
      "\n",
      "                                       caption_clean  \n",
      "0  #TrumpIsUnfitForOffice #ArlingtonNationalCemet...  \n",
      "1   I would not be surprised if he did this befor...  \n",
      "2  Respect marriage. Donald Trump and Where's Mel...  \n",
      "3  Pray for Trump. He longs for North Korea and h...  \n",
      "4  Pray for Trump. He joined Hamas. ðŸ‡ºðŸ‡¸ #trumpmelt...  \n",
      "5                                #KamalaWinswithJoy   \n",
      "6   Trump has used everyone around him for his pe...  \n",
      "7                                                     \n",
      "8   #TrumpDishonorsUS #TrumpIsUnfitForOffice Let'...  \n",
      "9                               Cop vs the felon. ðŸ˜‚   \n",
      "\n",
      "[10 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Extract and remove mentions\n",
    "df['mentions'] = df['caption'].apply(lambda x: ' '.join(re.findall(r'@\\w+', x)))\n",
    "df['caption_no_mentions'] = df['caption'].apply(lambda x: re.sub(r'@\\w+', '', x))\n",
    "\n",
    "# Extract hashtags\n",
    "df['new_hashtags'] = df['caption_no_mentions'].apply(lambda x: ' '.join(re.findall(r'#\\w+', x)))\n",
    "df['hashtags'] = df['hashtags'].mask(df['hashtags'].isna() | (df['hashtags'] == ''), df['new_hashtags'])\n",
    "df.drop(columns=['new_hashtags'], inplace=True)\n",
    "\n",
    "# Extract and remove URLs\n",
    "df['urls'] = df['caption_no_mentions'].apply(lambda x: ' '.join(re.findall(r'http\\S+', x)))\n",
    "df['urls'] = df['urls'].str.replace(r'https://t\\.co/\\w+', '', regex=True)\n",
    "df['caption_clean'] = df['caption_no_mentions'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "\n",
    "#remove a ton of extra white space from caption_clean\n",
    "df['caption_clean'] = df['caption_clean'].str.replace(r'\\s+', ' ', regex=True)\n",
    "#drop filtering collumns\n",
    "df.drop(columns=['caption_no_mentions'], inplace=True)\n",
    "df.rename(columns={'caption': 'original_caption'}, inplace=True)\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Post ID                                       URL to tweet  \\\n",
      "0  1828837775791341997  https://x.com/JimmyMatthewsJr/status/182883777...   \n",
      "1  1828607916892422511  https://x.com/JimmyMatthewsJr/status/182860791...   \n",
      "2  1826979750856049026  https://x.com/TheVladsven/status/1826979750856...   \n",
      "3  1822071851960692972  https://x.com/TheVladsven/status/1822071851960...   \n",
      "4  1822069995565883579  https://x.com/TheVladsven/status/1822069995565...   \n",
      "\n",
      "                    creation_date  likes  \\\n",
      "0  Wed Aug 28 16:51:18 +0000 2024     29   \n",
      "1  Wed Aug 28 01:37:55 +0000 2024     21   \n",
      "2  Fri Aug 23 13:48:10 +0000 2024      1   \n",
      "3  Sat Aug 10 00:45:56 +0000 2024     19   \n",
      "4  Sat Aug 10 00:38:34 +0000 2024      8   \n",
      "\n",
      "                                    original_caption  retweet_count  \\\n",
      "0  #TrumpIsUnfitForOffice #ArlingtonNationalCemet...             18   \n",
      "1  @thedesertginger I would not be surprised if h...              5   \n",
      "2  Respect marriage.  Donald Trump and Where's Me...              2   \n",
      "3  Pray for Trump.  He longs for North Korea and ...              3   \n",
      "4  Pray for Trump. He joined Hamas.   ðŸ‡ºðŸ‡¸ #trumpme...              2   \n",
      "\n",
      "   comment_count  view_count    poster_handle        poster_display_name  ...  \\\n",
      "0              5        1793  JimmyMatthewsJr             Jimmy Matthews  ...   \n",
      "1              1         863  JimmyMatthewsJr             Jimmy Matthews  ...   \n",
      "2              2        1108      TheVladsven  Darth VladSven the Cat VI  ...   \n",
      "3              4        3808      TheVladsven  Darth VladSven the Cat VI  ...   \n",
      "4              2        2869      TheVladsven  Darth VladSven the Cat VI  ...   \n",
      "\n",
      "                                        comment_file  image+caption_target  \\\n",
      "0  Twitter_Scrape\\Target_Trump\\Anti\\Comments\\eat_...            anti-trump   \n",
      "1  Twitter_Scrape\\Target_Trump\\Anti\\Comments\\vete...            anti-trump   \n",
      "2                                                NaN            anti-trump   \n",
      "3  Twitter_Scrape\\Target_Trump\\Anti\\Comments\\trum...            anti-trump   \n",
      "4  Twitter_Scrape\\Target_Trump\\Anti\\Comments\\fore...            anti-trump   \n",
      "\n",
      "   political_leaning          mentions urls  \\\n",
      "0                  D                          \n",
      "1                  D  @thedesertginger        \n",
      "2                  D                          \n",
      "3                  D                          \n",
      "4                  D                          \n",
      "\n",
      "                                       caption_clean Positive Neutral  \\\n",
      "0  #TrumpIsUnfitForOffice #ArlingtonNationalCemet...    0.000   1.000   \n",
      "1   I would not be surprised if he did this befor...    0.000   0.868   \n",
      "2  Respect marriage. Donald Trump and Where's Mel...    0.274   0.679   \n",
      "3  Pray for Trump. He longs for North Korea and h...    0.141   0.859   \n",
      "4  Pray for Trump. He joined Hamas. ðŸ‡ºðŸ‡¸ #trumpmelt...    0.204   0.796   \n",
      "\n",
      "  Negative Compound  \n",
      "0    0.000   0.0000  \n",
      "1    0.132  -0.1695  \n",
      "2    0.047   0.8658  \n",
      "3    0.000   0.3182  \n",
      "4    0.000   0.3182  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Jonah\n",
      "[nltk_data]     Dalton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Perform sentiment analysis on the 'Text in English language' column and save the results to a new column\n",
    "def analyze_sentiment(text):\n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "    return sentiment_scores['pos'], sentiment_scores['neu'], sentiment_scores['neg'], sentiment_scores['compound']\n",
    "\n",
    "# Apply the analyze_sentiment function to the 'Text in English language' column\n",
    "df['Positive'], df['Neutral'], df['Negative'], df['Compound'] = zip(*df['caption_clean'].apply(analyze_sentiment))\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify the sentiment analysis\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Spacy Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: pip in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (24.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-25.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (75.5.0)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-75.8.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: wheel in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (0.45.0)\n",
      "Collecting wheel\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading pip-25.0-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/1.8 MB 14.5 MB/s eta 0:00:00\n",
      "Downloading setuptools-75.8.0-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 8.9 MB/s eta 0:00:00\n",
      "Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Installing collected packages: wheel, setuptools, pip\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.45.0\n",
      "    Uninstalling wheel-0.45.0:\n",
      "      Successfully uninstalled wheel-0.45.0\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 75.5.0\n",
      "    Uninstalling setuptools-75.5.0:\n",
      "      Successfully uninstalled setuptools-75.5.0\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.3.1\n",
      "    Uninstalling pip-24.3.1:\n",
      "      Successfully uninstalled pip-24.3.1\n",
      "Successfully installed pip-25.0 setuptools-75.8.0 wheel-0.45.1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (3.8.2)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.4-cp312-cp312-win_amd64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.4-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from spacy) (0.13.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from spacy) (4.67.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from spacy) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from spacy) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from spacy) (3.4.1)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.2.0-cp312-cp312-win_amd64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jonah dalton\\data_pipeline\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Downloading spacy-3.8.4-cp312-cp312-win_amd64.whl (11.8 MB)\n",
      "   ---------------------------------------- 0.0/11.8 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 3.9/11.8 MB 26.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.7/11.8 MB 23.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.8/11.8 MB 21.7 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.4-cp312-cp312-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 25.4 MB/s eta 0:00:00\n",
      "Downloading blis-1.2.0-cp312-cp312-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   -------------------------------------- - 6.0/6.3 MB 30.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 27.4 MB/s eta 0:00:00\n",
      "Installing collected packages: blis, thinc, spacy\n",
      "  Attempting uninstall: blis\n",
      "    Found existing installation: blis 1.0.1\n",
      "    Uninstalling blis-1.0.1:\n",
      "      Successfully uninstalled blis-1.0.1\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.3.2\n",
      "    Uninstalling thinc-8.3.2:\n",
      "      Successfully uninstalled thinc-8.3.2\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.8.2\n",
      "    Uninstalling spacy-3.8.2:\n",
      "      Successfully uninstalled spacy-3.8.2\n",
      "Successfully installed blis-1.2.0 spacy-3.8.4 thinc-8.3.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ------------ --------------------------- 3.9/12.8 MB 26.2 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.2/12.8 MB 29.0 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 25.1 MB/s eta 0:00:00\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "%pip install -U pip setuptools wheel\n",
    "%pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to perform NER and separate entities by type\n",
    "def extract_entities_by_type(text):\n",
    "    doc = nlp(text)\n",
    "    persons = [entity.text for entity in doc.ents if entity.label_ == \"PERSON\"]\n",
    "    orgs = [entity.text for entity in doc.ents if entity.label_ == \"ORG\"]\n",
    "    gpes = [entity.text for entity in doc.ents if entity.label_ == \"GPE\"]\n",
    "    rel = [entity.text for entity in doc.ents if entity.label_ == \"NORP\"]\n",
    "    dat = [entity.text for entity in doc.ents if entity.label_ == \"DATE\"]\n",
    "    return persons, orgs, gpes, rel, dat\n",
    "\n",
    "# Apply the function to each tweet and store the results in separate columns\n",
    "df[['Persons', 'Organizations', 'Locations', 'Nationalities/religiious groups', 'Date']] = df['caption_clean'].apply(\n",
    "    lambda x: pd.Series(extract_entities_by_type(x))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0 (Patriotism & Trump Support Network): remember, lets, trumpisunfitforoffice, maga, great, happy, war, think, support, fight\n",
      "Weights: [3.5112127  2.32282135 2.24065639 2.1360738  1.54219845 1.52615314\n",
      " 0.92880969 0.91828296 0.90672634 0.90497616]\n",
      "Topic #1 (MAGA & Election Enthusiasm): got, ve, fuck, big, wait, november, babies, voting, votetrump2024tosaveamerica, proud\n",
      "Weights: [1.58830027 1.58336951 1.53716691 0.92613291 0.9233155  0.91821055\n",
      " 0.90222693 0.87026681 0.85767267 0.80575576]\n",
      "Topic #2 (Voting & Opposition to Kamala Harris): patriots, vote, trumpvance2024tosaveamerica, maga, thanks, president, voted, follow, january, want\n",
      "Weights: [6.7202935  4.71532963 3.31449109 3.04678504 2.95653657 2.94591133\n",
      " 2.6740844  2.30380885 2.24764323 2.23219433]\n",
      "Topic #3 (Voter Readiness & Motivation): maga, maga2024, trump2024, kamala, vote, don, trump2024vance, real, forget, repost\n",
      "Weights: [10.96396811  8.46139658  5.33786982  5.21818483  3.31829418  2.99471261\n",
      "  2.97477014  2.64618641  2.31917317  2.28979749]\n",
      "Topic #4 (Canadites Foreign Loyalty): trump, donald, maga, america, trump2024, election2024, like, day, president, iran\n",
      "Weights: [51.21502719 10.18078513 10.01294038  9.54535185  8.43470674  7.18515573\n",
      "  7.17714591  6.66474929  6.61979954  6.4769034 ]\n",
      "Topic #5 (Harris Criticism & Justice Themes): harris, ifbap, friends, say, black, grow, love, follow, united, let\n",
      "Weights: [5.69671009 5.67093727 4.33559721 3.77602717 3.72532926 3.65207518\n",
      " 3.02699589 3.02189724 2.99739488 2.85792366]\n",
      "Topic #6 (Trump Loyalty & Enthusiasm): maga, trump, trump2024, vote, new, trumpvance2024, fight, women, amp, protect\n",
      "Weights: [8.95088384 7.39558818 7.08996075 5.68872613 5.49452346 4.68463554\n",
      " 4.43455858 4.3942989  4.3457076  4.27871471]\n",
      "                                    original_caption  \\\n",
      "0  #TrumpIsUnfitForOffice #ArlingtonNationalCemet...   \n",
      "1  @thedesertginger I would not be surprised if h...   \n",
      "2  Respect marriage.  Donald Trump and Where's Me...   \n",
      "3  Pray for Trump.  He longs for North Korea and ...   \n",
      "4  Pray for Trump. He joined Hamas.   ðŸ‡ºðŸ‡¸ #trumpme...   \n",
      "5         #KamalaWinswithJoy https://t.co/k5dnunB1zQ   \n",
      "6  @VetsForRL Trump has used everyone around him ...   \n",
      "7  @ZionDarkwood @SteveLovesAmmo https://t.co/rJu...   \n",
      "8  @SteveSchmidtSES #TrumpDishonorsUS #TrumpIsUnf...   \n",
      "9        Cop vs the felon. ðŸ˜‚ https://t.co/LfXb1J7bBA   \n",
      "\n",
      "                          topic_label  \n",
      "0  Patriotism & Trump Support Network  \n",
      "1   Harris Criticism & Justice Themes  \n",
      "2           Canadites Foreign Loyalty  \n",
      "3           Canadites Foreign Loyalty  \n",
      "4           Canadites Foreign Loyalty  \n",
      "5  Patriotism & Trump Support Network  \n",
      "6  Patriotism & Trump Support Network  \n",
      "7  Patriotism & Trump Support Network  \n",
      "8  Patriotism & Trump Support Network  \n",
      "9  Patriotism & Trump Support Network  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Preprocess text function (removes punctuation)\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  # Ensure the text is a string\n",
    "        text = text.lower()  # Convert text to lowercase\n",
    "        text = ''.join([char for char in text if char not in string.punctuation])  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "df['topic_model_clean'] = df['caption_clean'].apply(preprocess_text)\n",
    "\n",
    "# **Replace TF-IDF with CountVectorizer**\n",
    "vectorizer = CountVectorizer(stop_words='english', max_df=0.93, min_df=3)\n",
    "X = vectorizer.fit_transform(df['topic_model_clean'])\n",
    "\n",
    "# **Train LDA Model**\n",
    "num_topics = 7\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topics, max_iter=10, learning_method='online', random_state=42)\n",
    "lda_model.fit(X)\n",
    "\n",
    "# Get top words for each topic\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "n_top_words = 10\n",
    "topic_keywords = {}\n",
    "\n",
    "topics = {\n",
    "    0: \"Patriotism & Trump Support Network\",\n",
    "    1: \"MAGA & Election Enthusiasm\",\n",
    "    2: \"Voting & Opposition to Kamala Harris\",\n",
    "    3: \"Voter Readiness & Motivation\",\n",
    "    4: \"Canadites Foreign Loyalty\",\n",
    "    5: \"Harris Criticism & Justice Themes\",\n",
    "    6: \"Trump Loyalty & Enthusiasm\",\n",
    "    7: \"Elections & Trump Presidency\"\n",
    "}\n",
    "\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    top_words = [terms[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "    topic_keywords[topic_idx] = top_words\n",
    "    print(f\"Topic #{topic_idx} ({topics[topic_idx]}): {', '.join(top_words)}\")\n",
    "    print(f\"Weights: {topic[topic.argsort()[:-n_top_words - 1:-1]]}\")\n",
    "\n",
    "# **Assign Topics to Each Document**\n",
    "topic_assignments = lda_model.transform(X)\n",
    "df['topic_id'] = topic_assignments.argmax(axis=1)\n",
    "\n",
    "# **Map Topics to Labels**\n",
    "df['topic_label'] = df['topic_id'].map(topics)\n",
    "\n",
    "# Remove processed text column\n",
    "df.drop('topic_model_clean', axis=1, inplace=True)\n",
    "\n",
    "# Print the first 10 rows with assigned topics\n",
    "print(df[['original_caption', 'topic_label']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the New CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the required columns for the new CSV file\n",
    "\n",
    "new_df = df[['caption_clean', 'image+caption_target', 'political_leaning', 'topic_label', 'mentions', 'hashtags',  'urls', 'likes', 'retweet_count', 'comment_count', 'view_count', 'poster_follower_count', 'poster_frequency', 'total_posts', \n",
    "    'image_filename', 'comment_file', 'Positive', 'Neutral', 'Negative', 'Compound', 'Persons', 'Organizations', 'Locations', 'Nationalities/religiious groups', 'Date',  'Post ID', 'URL to tweet', 'creation_date', 'poster_handle', 'poster_display_name',]]\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "new_df.to_csv('Processed_Tweets.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
